<!doctype html>
<html lang="it">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Num-Aut-Imag - ONNX Runtime Web (fix wasm)</title>
  <base href="/Num-Aut-Imag/">
  <style>
    body { font-family: Roboto, Arial, sans-serif; padding: 1rem; }
    pre { background:#f5f5f5; padding:1rem; border-radius:6px; }
    .error { color: #8b0000; }
  </style>
</head>
<body>
  <h1>Num-Aut-Imag — Test ONNX Runtime Web (fix wasm)</h1>
  <p>Questa pagina carica <code>onnxruntime-web</code> dal CDN e forza il caricamento del file .wasm corrispondente dal CDN.</p>

  <p><b>Modifica <code>modelUrl</code> sotto</b> con il percorso appropriato al tuo modello .onnx (relativo alla repo o assoluto).</p>
  <pre id="log"></pre>

  <!-- ONNX Runtime Web (minified) from jsDelivr -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script>
    const logEl = document.getElementById('log');
    function log(...args){
      console.log(...args);
      logEl.textContent += args.map(a=> (typeof a === 'object' ? JSON.stringify(a,null,2) : String(a))).join(' ') + '\n';
    }

    (async function main(){
      try{
        // Indirizzo del modello .onnx
        const modelUrl = 'https://raw.githubusercontent.com/ultralytics/assets/main/yolov8n.onnx';

        // Forziamo il caricamento del runtime wasm dal CDN jsDelivr
        if (typeof ort !== 'undefined' && ort.env && ort.env.wasm) {
          ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/wasm/ort-wasm.wasm';
          log('Impostato ort.env.wasm.wasmPaths su:', ort.env.wasm.wasmPaths);
        } else {
          log('onnxruntime-web non è stato caricato correttamente.');
        }

        // Proviamo prima backend WASM; fallback a WebGL
        try{
          log('Tentativo di creare sessione con backend WASM...');
          const session = await ort.InferenceSession.create(modelUrl, {executionProviders: ['wasm']});
          log('Sessione creata (wasm):', session);
          alert('Model caricato con successo usando backend WASM.');
        } catch (wasmErr) {
          log('Errore backend WASM:', wasmErr);
          try{
            log('Tentativo fallback a WebGL...');
            const session2 = await ort.InferenceSession.create(modelUrl, {executionProviders: ['webgl']});
            log('Sessione creata (webgl):', session2);
            alert('Model caricato con successo usando backend WebGL (fallback).');
          } catch (webglErr) {
            log('Errore fallback WebGL:', webglErr);
            const userMessage = 'Errore caricamento modello: Nessun backend disponibile.';
            log(userMessage);
            alert(userMessage + '\nControlla Console e Network.');
          }
        }
      } catch (e) {
        log('Errore inaspettato durante l\'avvio:', e);
        alert('Errore inaspettato (vedi console).');
      }
    })();
  </script>

  <hr />
  <p class="error">Note:
    <ul>
      <li>Assicurati che il file .onnx sia accessibile sulla rete.</li>
      <li>Se usi un link raw di GitHub, assicurati che i permessi CORS siano corretti.</li>
      <li>Altrimenti carica il file nella tua repo e usa un percorso relativo.</li>
    </ul>
  </p>
</body>
</html>
